# big-file 大文件上传

参考[请你实现一个大文件上传和断点续传](https://juejin.im/post/6844904046436843527#heading-6)

从零搭建前端和服务端，实现一个大文件上传和断点续传的 demo

前端：vue 

服务端：nodejs

## 起步
```
# 安装依赖
yarn
# 启动服务
yarn serve
# 启动前端
yarn start
```

### 思路

利用`Blob.prototype.slice` 方法将文件分成`n`个切片, 每个切片发送个上传请求, 

等所有请求完成, 再发送个合并请求, 通知服务端合并之前上传的切片

服务端接收这些切片,先缓存到一个文件夹中, 待接到合并请求后,

使用 nodejs 的 读写流（readStream/writeStream），将所有切片的流传输到最终文件的流里

合并完后删除缓存文件夹

## 版本

每个版本建立个独立分支, 方便按块学习

### 一. 初步完成文件切片上传

使用slice()将文件切片后使用Promise.all()分切片上传, 

服务端接收 使用multiparty 解析传过来的文件, 切片上传完后, 发送切片合并请求, 服务端合并切片, 完成这个流程

### 二. 基本进度条完成

引入element组件来做进度条展示

使用xhr.upload.onprogress 事件来监听每个切片上传进度情况, 根据每个进度和总的文件大小计算总进度

### 三. 使用hash 实现秒传

避免修改文件名后失去效果, 所以要根据文件内容生成 hash, 之后无论怎么修改文件名, 只要文件内容不变, 缓存都会有效

这里使用spark-md5实现生成文件内容hash值, 但读取文件内容计算hash非常耗时, 会引起ui阻塞,使页面假死, 这里使用web-worker 在 worker 线程计算 hash

在public放入spark-md5.min.js库, 新建hash.js文件用来生成hash, 点击上传时生成hash

把之前用文件名的替换成hash, 包括缓存切片文件夹名, 切片名, 合并文件名等

这样在前端发起上传请求时, 就可以根据hash判断该文件是否存在, 存在就直接返回, 无需再次上传

在发送上传请求前, 先发送一个校验请求, 校验文件是否存在, 存在就直接返回, 无需再次上传

### 四. 断点续传

断点续传的原理在于前端/服务端需要记住已上传的切片，这样下次上传就可以跳过之前已上传的部分

我们使用服务端保存已上传的切片 hash，前端每次上传前向服务端获取已上传的切片

我们用一个数组存储发送请求的xhr, 点击暂停,调用XMLHttpRequest 的 abort 方法取消请求, 改造下封装的请求

点击继续上传, 先向服务端发送个验证请求, 获取之前有多少切片已经上传成功, 再次上传, 跳过这些已经上传好的

最后要改造下进度条, 和禁用按钮逻辑

在上传的时候不能再点击上传按钮, 因为点击上传按钮会: 重新生成切片并再次计算文件hash, 所以在上传过程中要禁用掉

进度条:

切片进度条: 由于在点击上传/恢复上传时，会调用验证接口返回已上传的切片，所以需要将已上传切片的进度变成 100%

总进度条: 点击暂停会取消并清空切片的 xhr 请求，此时如果已经上传了一部分，就会发现文件进度条有倒退的现象

所以弄个变量记录目前已经上传了多少, 当再次继续上传时的总进度超过之前保存的进度, 就更新进度

## 总结

 大文件上传: 

   * 使用node内置模块http编写后端接口
   * 服务端使用multiparty解析前端传过来的文件数据。
   * 前端使用Blob.prototype.slice 将文件切成约定大小的切片, 通过Promiss.all发送所有请求, 当所有返回成功再向服务端发送合并请求
   * 服务端接收切片, 将切片放到一个文件夹中, 收到合并请求后把这些切片合并起来, 完成后删除该文件夹
   * 使用 XMLHttpRequest 的 upload.onprogress 方法监听上传进度

 断点续传

   * 使用spark-md5解析文件生成hash, 使用hash保存切片和文件
   * 后端根据hash判断该切片或文件是否已经上传
   * 使用 XMLHttpRequest 的 abort 取消上传
   * 服务端根据hash获取已经上传的切片,返回已经上传的切片列表, 跳过这些切片继续上传

